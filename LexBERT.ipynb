{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4749e8e-58fe-404a-a46d-d6663a7c695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForPreTraining, pipeline, Trainer, TrainingArguments\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52d8f4e9-5edb-4b78-ba7d-ab285fd5a2a0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json from cache at /Users/emircan/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dbmdz/bert-base-turkish-uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/vocab.txt from cache at /Users/emircan/.cache/huggingface/transformers/79be37af8c4dff9ec826fae8061b4e805ea5b81e12ac77fd93021d445d90d46b.4ff1dc178b93b6275853fbacc77602ab780339fca423db0c4327e3f03209a772\n",
      "loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/tokenizer_config.json from cache at /Users/emircan/.cache/huggingface/transformers/9cb640d980c71e1b39f560fec126669724429503421cb2df1d82afee38e1ada1.1234e3020e8b22f6151b88ea98a593213c8b28579933530baa777c65097a4e37\n",
      "loading configuration file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json from cache at /Users/emircan/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dbmdz/bert-base-turkish-uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json from cache at /Users/emircan/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dbmdz/bert-base-turkish-uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json from cache at /Users/emircan/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dbmdz/bert-base-turkish-uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/pytorch_model.bin from cache at /Users/emircan/.cache/huggingface/transformers/2cde030ef941e010ac168a81f127fc4dc920c9402403f7adf8377728e30d898e.a4faeaa8e5e3c20f3a8a00668f2250edfb32c44bda6d7b9dc9e75aaddef93dcc\n",
      "All model checkpoint weights were used when initializing BertForPreTraining.\n",
      "\n",
      "All the weights of BertForPreTraining were initialized from the model checkpoint at dbmdz/bert-base-turkish-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForPreTraining for predictions without further training.\n",
      "loading configuration file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json from cache at /Users/emircan/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dbmdz/bert-base-turkish-uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json from cache at /Users/emircan/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dbmdz/bert-base-turkish-uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/pytorch_model.bin from cache at /Users/emircan/.cache/huggingface/transformers/2cde030ef941e010ac168a81f127fc4dc920c9402403f7adf8377728e30d898e.a4faeaa8e5e3c20f3a8a00668f2250edfb32c44bda6d7b9dc9e75aaddef93dcc\n",
      "Some weights of the model checkpoint at dbmdz/bert-base-turkish-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at dbmdz/bert-base-turkish-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('dbmdz/bert-base-turkish-uncased')\n",
    "model = AutoModelForPreTraining.from_pretrained('dbmdz/bert-base-turkish-uncased', output_hidden_states=True)\n",
    "pipe = pipeline('fill-mask', model='dbmdz/bert-base-turkish-uncased', tokenizer=tokenizer, framework='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa024d29-c351-484b-bba5-fea7ac57b503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Resmî Gazete Tarihi: 11.07.2012 Resmî Gazete Sayısı: 28350\\nKanun No. 6338 Kabul Tarihi : 29/06/2012\\nBÜTÇE KANUNLARINDA YER ALAN BAZI HÜKÜMLERİN İLGİLİ KANUN VE KANUN HÜKMÜNDE KARARNAMELERE EKLENMESİNE DAİR KANUN\\n\\nMadde 1 – 5/1/1961 tarihli ve 237 sayılı Taşıt Kanununun 10 uncu maddesine aşağıdaki fıkralar eklenmiştir.\\n\\n“Türk Silahlı Kuvvetleri (Jandarma Genel Komutanlığı ve Sahil Güvenlik Komutanlığı dâhil), Emniyet Genel Müdürlüğü ile Gümrük ve Ticaret Bakanlığı Gümrükler Muhafaza Genel Müdürlüğüne kurum, kuruluş, dernek ve vakıflarca hibe edilecek taşıtlar, merkezî yönetim bütçe kanununa bağlı (T) işaretli cetvelde gösterilmesine gerek bulunmaksızın Bakanlar Kurulu kararı ile edinilebilir.\\n\\nEmniyet Genel Müdürlüğüne ait taşıtlar, 12/4/2001 tarihli ve 4645 sayılı Emniyet Genel Müdürlüğüne Ait Araç, Gereç, Mal ve Malzemenin Satış, Hibe, HEK ve Hurda Durum ve İşlemleri ile Hizmet Satışına Dair Kanun hükümleri çerçevesinde merkezî yönetim bütçe kanununa bağlı (T) işaretli cetvelde gösterilmelerine gerek bulunmaksızın, cinsi ve adedi İçişleri Bakanlığının talebi ve Maliye Bakanlığının teklifi üzerine alınacak Bakanlar Kurulu kararında belirlenmek kaydıyla 4/1/2002 tarihli ve 4734 sayılı Kamu İhale Kanununa tabi olmaksızın yenileri ile mübadele edilebilir. Aradaki fiyat farkı, döner sermaye gelirleri, sosyal tesis veya kantin gelirleri ile Türk Polis Teşkilatını Güçlendirme Vakfı gelirlerinden karşılanır.\\n\\nTürk Silahlı Kuvvetlerine (Jandarma Genel Komutanlığı ve Sahil Güvenlik Komutanlığı dâhil) ait taşıtlar, 30/5/1985 tarihli ve 3212 sayılı Silahlı Kuvvetler İhtiyaç Fazlası Mal ve Hizmetlerinin Satış, Hibe, Devir ve Elden Çıkarılması; Diğer Devletler Adına Yurt Dışı ve Yurt İçi Alımların Yapılması ve Eğitim Görecek Yabancı Personel Hakkında Kanun hükümleri çerçevesinde merkezî yönetim bütçe kanununa bağlı (T) işaretli cetvelde gösterilmelerine gerek bulunmaksızın, cinsi ve adedi Milli Savunma Bakanlığının (Jandarma Genel Komutanlığı ve Sahil Güvenlik Komutanlığı için ise İçişleri Bakanlığının) talebi ve Maliye Bakanlığının teklifi üzerine çıkarılacak Bakanlar Kurulu kararında belirlenmek kaydıyla 4734 sayılı Kanuna tabi olmaksızın yenileri ile mübadele edilebilir. Aradaki fiyat farkı, sosyal tesis veya kantin gelirleri ile döner sermaye gelirlerinden veya bağış yoluyla (Jandarma Genel Komutanlığı için Jandarma Asayiş Vakfı gelirlerinden) karşılanır.\\n\\nTürk Silahlı Kuvvetlerine (Jandarma Genel Komutanlığı ve Sahil Güvenlik Komutanlığı dâhil) ait taşıtlardan, trafiğe tescil tarihi itibarıyla en az on yaşını doldurmuş olanlar, merkezî yönetim bütçe kanununa bağlı (T) işaretli cetvelde gösterilmelerine gerek bulunmaksızın, cinsi ve adedi, ilgisine göre Milli Savunma Bakanlığı veya İçişleri Bakanlığının kararıyla, 4734 sayılı Kanuna tabi olmaksızın ve satılacak taşıt sayısı satın alınacak taşıt sayısından az olmamak ve satın alınacak taşıt sayısı merkezî yönetim bütçe kanununda belirtilen adetleri geçmemek üzere yenileri ile mübadele edilebilir ve aradaki fiyat farkı, bütçeden karşılanabilir. Bu fıkrada yer almayan hususlar hakkında 3212 sayılı Kanun hükümleri uygulanır.”\\n\\nMadde 2 – 237 sayılı Kanunun 17 nci maddesine aşağıdaki fıkra eklenmiştir.\\n\\n“Vakıf, dernek, sandık, banka, birlik, firma, şahıs ve benzeri kuruluş veya kişilere ait olup bu Kanun kapsamında bulunan kurumlar ile özel kanunla kurulmuş diğer kamu kurum, kurul, üst kurul ve kuruluşlarınca kullanılan taşıtların giderleri için (güvenlik hizmetlerinde kullanılan taşıtlar hariç) kurum bütçelerinden hiçbir şekilde ödeme yapılamaz.”\\n\\nMadde 3 – 14/7/1965 tarihli ve 657 sayılı Devlet Memurları Kanununun 156 ncı maddesinin birinci fıkrasında yer alan “aylık tutarından, alınacak vergi ve kanunlar gereğince yapılacak bütün kesintiler indirildikten sonra (Kefalet Sandığı kesintileri hariç) kalan kısmın,” ibaresi “brüt aylık tutarın,” şeklinde değiştirilmiş ve aynı maddenin ikinci fıkrası yürürlükten kaldırılmıştır.\\n\\nMadde 4 – 2/9/1983 tarihli ve 78 sayılı Yükseköğretim Kurumları Öğretim Elemanlarının Kadroları Hakkında Kanun Hükmünde Kararnamenin 5 inci maddesi aşağıdaki şekilde değiştirilmiştir.\\n\\n“MADDE 5- Öğretim üyeleri hariç, bu Kanun Hükmünde Kararnameye ekli (1) sayılı cetvelde yer alan boş öğretim elemanı kadrolarına açıktan veya yükseköğretim kurumları ile diğer kamu idare, kurum ve kuruluşlarından nakil suretiyle yapılabilecek toplam atama sayısı sınırı merkezî yönetim bütçe kanununda gösterilir.\\nTıpta ve Diş Hekimliğinde Uzmanlık Eğitimi Yönetmeliği uyarınca araştırma görevlisi kadrolarına yapılabilecek atamalar ile 8/4/1929 tarihli ve 1416 sayılı Ecnebi Memleketlere Gönderilecek Talebe Hakkında Kanun uyarınca yurt dışına eğitim amacıyla gönderilenlerden öğretim elemanı kadrolarına yapılabilecek atamalar merkezî yönetim bütçe kanununda öngörülen atama sınırlamalarına tabi değildir.\\n\\nÖğretim üyesi kadrolarından ayrılanların sayısı ile Tıpta ve Diş Hekimliğinde Uzmanlık Eğitimi Yönetmeliği uyarınca atanmış oldukları araştırma görevlisi kadrolarından ayrılanların sayısı merkezî yönetim bütçe kanununda öngörülen atama sayısının hesabında dikkate alınmaz.\\n\\nMerkezî yönetim bütçe kanununda belirtilen atama sayısının yükseköğretim kurumları itibarıyla dağılımı, kullanımı ve diğer hususlar Maliye Bakanlığının ve Devlet Personel Başkanlığının görüşleri üzerine Yükseköğretim Kurulunca belirlenir ve boş öğretim elemanı kadrolarına Yükseköğretim Kurulunun izni olmadıkça atama yapılamaz.”\\n\\nMadde 6 – 13/12/1983 tarihli ve 190 sayılı Genel Kadro ve Usulü Hakkında Kanun Hükmünde Kararnamenin 11 inci maddesinin birinci fıkrasının (d) bendine “kullanılması,” ibaresinden sonra gelmek üzere “22/5/2003 tarihli ve 4857 sayılı İş Kanununun 30 uncu maddesi uyarınca yükümlü oldukları özürlü ve eski hükümlü işçi atamaları hariç” ibaresi ile birinci cümleden sonra gelmek üzere aşağıdaki cümle eklenmiştir.\\n\\n“Özürlü ve eski hükümlü işçi atamaları, izleyen ayın sonuna kadar Maliye Bakanlığına ve Devlet Personel Başkanlığına bildirilir.”\\n\\nMadde 7 – 190 sayılı Kanun Hükmünde Kararnamenin ek 7 nci maddesi aşağıdaki şekilde değiştirilmiştir.\\n\\n“EK MADDE 7- Bu Kanun Hükmünde Kararnamenin 2 nci maddesinde belirtilen kamu idare, kurum ve kuruluşlarının; serbest memur kadrolarına açıktan veya diğer kamu idare, kurum ve kuruluşlarından nakil suretiyle yapabilecekleri toplam atama sayısı sınırı merkezî yönetim bütçe kanununda gösterilir.\\n\\nHâkimlik ve savcılık meslekleri ile bu meslekten sayılan görevlere ve Tıpta ve Diş Hekimliğinde Uzmanlık Eğitimi Yönetmeliği uyarınca asistan kadrolarına yapılacak atamalar, Maliye Bakanlığı, Çalışma ve Sosyal Güvenlik Bakanlığı, Gelir İdaresi Başkanlığı ve Sosyal Güvenlik Kurumu Başkanlığında münhasıran vergi ve sosyal güvenlik alanlarında istihdam edilecek yardımcı kadrolarına yapılacak atamalar, 657 sayılı Kanunun 53 üncü maddesine göre yapılacak özürlü personel atamaları, 27/7/1967 tarihli ve 926 sayılı Türk Silahlı Kuvvetleri Personel Kanunu kapsamında veya diğer ilgili mevzuata göre yapılacak askerî personel atamaları, emniyet hizmetleri sınıfında bulunan kadrolara yapılacak atamalar, 12/4/1991 tarihli ve 3713 sayılı Terörle Mücadele Kanununun ek 1 inci maddesi ve 24/5/1983 tarihli ve 2828 sayılı Sosyal Hizmetler Kanununun ek 1 inci maddesi uyarınca yapılacak atamalar ile 24/11/1994 tarihli ve 4046 sayılı Özelleştirme Uygulamaları Hakkında Kanunun 22 nci maddesi uyarınca yapılacak personel nakilleri merkezî yönetim bütçe kanununda öngörülen atama sınırlamalarına tabi değildir.\\n\\nSınırlamalara tabi olmaksızın atama yapılabileceği ikinci fıkrada belirtilen kadrolardan ayrılanların sayısı merkezî yönetim bütçe kanununda öngörülen atama sayısının hesabında dikkate alınmaz.\\n\\nMerkezî yönetim bütçe kanununda belirtilen atama sayısının kamu idare, kurum ve kuruluşları itibarıyla dağılımı, kullanımı ve diğer hususlar Devlet Personel Başkanlığının bağlı olduğu Bakan ve Maliye Bakanının müşterek teklifi üzerine Başbakan onayı ile belirlenir. Mali yıl içinde yeniden teşkilatlanan veya yeni kurulan kamu idare, kurum ve kuruluşları için merkezî yönetim bütçe kanununda belirlenen atama sayısının yüzde onunu geçmemek ve Personel Giderlerini Karşılama Ödeneği tertibindeki ödenek dikkate alınmak suretiyle ilave sayı tespit etmeye Maliye Bakanlığının teklifi üzerine Bakanlar Kurulu yetkilidir.\\n\\nBirinci fıkra kapsamında 657 sayılı Kanunun 59 uncu ve 92 nci maddeleri uyarınca yapılabilecek açıktan atamalar için Devlet Personel Başkanlığından izin alınması zorunludur.\\n\\nKamu idare, kurum ve kuruluşları, izleyen mali yıla ait ilave atama izin taleplerini içinde bulunulan mali yılın ağustos ayı sonuna kadar Maliye Bakanlığına ve Devlet Personel Başkanlığına bildirir.\\n\\n5018 sayılı Kanuna ekli cetvellerde yer alan kamu idareleri ile bu Kanun Hükmünde Kararnamenin 2 nci maddesinde belirtilen kamu idare, kurum ve kuruluşları kadro ve pozisyonlarının dolu-boş durumları ile bunlarda meydana gelen değişikliklere ilişkin bilgileri mart, haziran, eylül ve aralık aylarının son günleri itibarıyla düzenleyerek, anılan ayları izleyen ayın yirmisine kadar bütçe bilgi sistemi vasıtasıyla Maliye Bakanlığına bildirirler. Ayrıca, bu bilgileri içeren cetveller, Devlet Personel Başkanlığına gönderilir.”\\n\\nMadde – 27/6/1989 tarihli ve 375 sayılı Kanun Hükmünde Kararnamenin ek 7 nci maddesine aşağıdaki fıkralar eklenmiştir.\\n\\n“657 sayılı Kanunun 4 üncü maddesinin (B) fıkrası ile birinci fıkrada belirtilen mevzuat kapsamında, bir önceki mali yılda vizeli mevcut pozisyon ve tip sözleşme örnekleri yeni bir vize yapılmasına gerek kalmaksızın içinde bulunulan mali yılda da kullanılmaya devam olunur. Bu pozisyonlarda bir önceki mali yılda istihdam edilen personelden, içinde bulunulan mali yılda da görevlerine devam etmeleri uygun görülenlerle, mevcut sözleşme ücretlerine içinde bulunulan mali yıl için mevzuat uyarınca yapılacak artışlar ilave edilmek suretiyle yeni sözleşme yapılır.\\n\\nKanun, uluslararası anlaşma, Bakanlar Kurulu kararı veya yılı programıyla kurulması veya genişletilmesi öngörülen birimler ile hizmetin gerektirdiği zorunlu hâller için, yılı ödeneğini aşmamak kaydıyla yapılacak yeni vizeler dışında, bir önceki mali yıl sözleşmeli personel pozisyon sayıları hiçbir şekilde aşılamaz.”\\n\\nMadde 8 – 24/11/1994 tarihli ve 4046 sayılı Özelleştirme Uygulamaları Hakkında Kanunun 10 uncu maddesinin üçüncü fıkrası aşağıdaki şekilde değiştirilmiştir.\\n\\n“Özelleştirme Fonunun nakit fazlası, Fon tarafından Hazine İç Ödemeler Muhasebe Birimi hesaplarına aktarılır ve genel bütçenin (B) işaretli cetveline gelir kaydedilir. Özelleştirme Fonundan diğer herhangi bir fona aktarma yapılmaz.”\\n\\nMadde 9 – 28/3/2002 tarihli ve 4749 sayılı Kamu Finansmanı ve Borç Yönetiminin Düzenlenmesi Hakkında Kanunun 9 uncu maddesine aşağıdaki fıkra eklenmiştir.\\n\\n“Yurt dışı kaynaklardan hibe olarak yıl içinde elde edilecek imkânların Türk Lirası karşılıklarını Müsteşarlığın teklifi üzerine gereğine göre bütçeye gelir veya gelir-ödenek-gider kaydetmeye, dış kaynaklardan veya uluslararası antlaşmalarla bağış ve kredi yoluyla gelecek her çeşit malzemenin navlun ve dışalımla ilgili vergi ve resimlerinin ödenmesi amacı ile bunların karşılığını, ilgili bütçelerinde mevcut veya yeni açılacak tertiplere ödenek kaydetmeye ve gereken işlemleri yapmaya, Millî Savunma Bakanlığı, Jandarma Genel Komutanlığı ve Sahil Güvenlik Komutanlığı ihtiyaçları için yabancı devletlerden askerî yardım yoluyla veya diğer yollardan fiilen sağlanacak malzeme ve eşya bedellerini, genel bütçenin (B) işaretli cetveline gelir ve karşılıklarını da bu bütçelerde açılacak özel tertiplere ödenek ve gider kaydetmeye Maliye Bakanı yetkilidir.”\\n\\nMadde 10 – 10/12/2003 tarihli ve 5018 sayılı Kamu Malî Yönetimi ve Kontrol Kanununun 15 inci maddesinin başlığı “Merkezî yönetim bütçe kanununun kapsamı ve düzeni” şeklinde değiştirilmiş ve maddeye aşağıdaki fıkralar eklenmiştir.\\n\\n“Merkezî yönetim bütçe kanununun gider cetvelinin bölümleri, analitik bütçe sınıflandırmasına uygun olarak fonksiyonlar şeklinde düzenlenir. Fonksiyonlar birinci, ikinci, üçüncü ve dördüncü düzeyde alt fonksiyonlara ayrılır.\\n\\nİlgili mevzuatta giderlere ilişkin olarak yer alan “Fasıl ve bölüm” deyimleri fonksiyonel sınıflandırmanın birinci düzeyini, “Kesim” deyimi fonksiyonel sınıflandırmanın ikinci düzeyini, “Madde” deyimi fonksiyonel sınıflandırmanın üçüncü düzeyini, “Tertip” deyimi kurumsal, fonksiyonel ve finansman tipi kodların bütün düzeyleri ile ekonomik sınıflandırmanın ilk iki düzeyini, borç ödemeleri yönünden “ilgili hizmet tertibi” deyimi borç konusu hizmetlerin yürütüldüğü ilgili tertipleri ifade eder.”\\n\\nMadde 11 – 5018 sayılı Kanunun 20 nci maddesinin birinci fıkrasına (d) bendinden sonra gelmek üzere aşağıdaki (e) bendi eklenmiş ve mevcut bentler buna göre teselsül ettirilmiştir.\\n\\n“e) İlgili mevzuatına göre, yılı içinde hizmetin gerektirdiği hâllerde Maliye Bakanlığınca belirlenen usûl ve esaslar çerçevesinde merkezî yönetim kapsamındaki kamu idarelerinin bütçelerinde yeni tertipler, gelir kodları ve finansman kodları açılabilir.”\\n\\nMadde 12 – 5018 sayılı Kanunun 25 inci maddesine, ikinci fıkrasından sonra gelmek üzere aşağıdaki fıkra eklenmiştir.\\n\\n“Millî Savunma Bakanlığı, Jandarma Genel Komutanlığı ve Sahil Güvenlik Komutanlığı bütçelerinin mal ve hizmet alım giderlerine ilişkin tertiplerinde yer alan savunma sektörü, altyapı, inşa, iskân ve tesisleriyle NATO altyapı yatırımlarının gerektirdiği inşa ve tesisler ve bunlara ilişkin kamulaştırmalar ile stratejik hedef planı içinde yer alan alım ve hizmetler, Kalkınma Bakanlığının vizesine bağlı olmayıp yılı yatırım programına ek yatırım cetvellerinde yer almaz.”\\n\\nMadde 13 – 5018 sayılı Kanunun 31 inci maddesine, üçüncü fıkradan sonra gelmek üzere aşağıdaki fıkra eklenmiştir.\\n\\n“Yükseköğretim Kurulu ile üniversiteler ve yüksek teknoloji enstitülerinde, harcama yetkilileri ödenek gönderme belgesiyle belirlenir. Bu idarelerde ödenek gönderme belgesi ile ödenek gönderilen birimler harcama birimi, kendisine ödenek gönderilen birimin en üst yöneticisi ise harcama yetkilisidir. Bütçe ödeneklerinin ilgili birimlere dağılımının planlanması, ödenek gönderme belgesine bağlanması ve kullanılmasına ilişkin usûl ve esaslar Maliye Bakanlığı tarafından belirlenir.”\\n\\nMadde 14 – 5018 sayılı Kanunun 51 inci maddesine, birinci fıkradan sonra gelmek üzere aşağıdaki fıkra eklenmiştir.\\n\\n“Tahakkuk ettirilecek giderler Devlet muhasebesi kayıtlarında ekonomik sınıflandırmanın dördüncü düzeyini de kapsayacak şekilde gösterilir.”\\n\\nMadde 15 – 5018 sayılı Kanunun 79 uncu maddesi aşağıdaki şekilde değiştirilmiştir.\\n\\n“MADDE 79- Özel mevzuatındaki hükümler saklı kalmak üzere, idare hesaplarında kayıtlı olup 21/7/1953 tarihli ve 6183 sayılı Amme Alacaklarının Tahsil Usulü Hakkında Kanun kapsamında izlenen kamu alacakları dışında kalan;\\n\\na) Zarurî veya mücbir sebeplerle takip ve tahsil imkânı kalmayan,\\n\\nb) Tahsili için yapılacak takibat giderlerinin asıl alacak tutarından fazla olacağı anlaşılan,\\nkamu alacaklarından merkezî yönetim bütçe kanununda gösterilen tutarlara kadar olanların kayıtlardan çıkarılmasına üst yöneticiler yetkilidir. (a) bendine göre belirlenen tutarı aşan kamu alacaklarından silinmesi öngörülenler merkezî yönetim bütçe kanununda ayrıca gösterilir.”\\n\\nMadde 16 – 3/7/2005 tarihli ve 5393 sayılı Belediye Kanununun 49 uncu maddesinin beşinci fıkrasına aşağıdaki cümle eklenmiştir.\\n\\n“Dördüncü fıkrada sayılan unvanlara ilişkin hizmetler dışında kalmak ve o hizmet için ihdas edilmiş kadro bulunmamak kaydıyla, İçişleri Bakanlığınca üçüncü fıkra çerçevesinde sözleşmeli personel istihdamı uygun görülmüş olan kadro unvanlarına ilişkin görevlerde, 657 sayılı Kanunun 4 üncü maddesinin (B) fıkrasına göre münhasıran kısmi süreli olarak sözleşmeli personel çalıştırılabilir.”\\n\\nMadde 17 – a) 24/11/1994 tarihli ve 4046 sayılı Özelleştirme Uygulamaları Hakkında Kanunun 2 nci maddesinin birinci fıkrasının (c) bendi ile geçici 23 üncü ve geçici 24 üncü maddeleri yürürlükten kaldırılmıştır.\\n\\nb) 20/2/2001 tarihli ve 4628 sayılı Elektrik Piyasası Kanununun 14 üncü maddesinin beşinci fıkrası yürürlükten kaldırılmıştır.\\n\\nMadde 18 – Bu Kanun 1/1/2013 tarihinde yürürlüğe girer.\\n\\nMadde 19 – Bu Kanun hükümlerini Bakanlar Kurulu yürütür.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds2 = [i['train'] for i in ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44f76dbf-87bc-45c6-95ee-1535a46bb7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using custom data configuration default-4053a65396a46b38\n",
      "Reusing dataset csv (/Users/emircan/.cache/huggingface/datasets/csv/default-4053a65396a46b38/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "417d9ffd5c2547a58727e76d9edeb5b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m ds \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m'\u001b[39m, data_files\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmevzuat.csv\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, train_dataset\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbugün hava çok güzel\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dev/lib/python3.10/site-packages/transformers/trainer.py:1317\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1312\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1314\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1316\u001b[0m )\n\u001b[0;32m-> 1317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dev/lib/python3.10/site-packages/transformers/trainer.py:1528\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1525\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_rng_state(resume_from_checkpoint)\n\u001b[1;32m   1527\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1528\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1529\u001b[0m \n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;66;03m# Skip past any already trained steps if resuming training\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m steps_trained_in_current_epoch \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1532\u001b[0m         steps_trained_in_current_epoch \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dev/lib/python3.10/site-packages/torch/utils/data/dataloader.py:576\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 576\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dev/lib/python3.10/site-packages/torch/utils/data/dataloader.py:616\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    615\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 616\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    618\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dev/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dev/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "TrainingArguments('LexBERT')\n",
    "ds = load_dataset('csv', data_files=['mevzuat.csv'])\n",
    "\n",
    "\n",
    "trainer = Trainer(model=model, tokenizer=tokenizer, train_dataset={'train':'bugün hava çok güzel'})\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "032654cf-fb53-47cf-a6a9-748f995c57af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset rotten_tomatoes (/Users/emircan/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46)\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset('rotten_tomatoes', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5373eaf-5d55-47a0-8056-bd3941b86831",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset rotten_tomatoes (/Users/emircan/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46)\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset('rotten_tomatoes', split='train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6eed2679-8e4a-4106-b858-022813f7295b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ed7f960e-a418-4558-a223-6b362f37dbaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-4053a65396a46b38\n",
      "Reusing dataset csv (/Users/emircan/.cache/huggingface/datasets/csv/default-4053a65396a46b38/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12179ba4d8645a387ca24502a85c7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy='epoch', seed=42)\n",
    "dataset = load_dataset('csv', data_files = 'mevzuat.csv')\n",
    "trainer = Trainer(model=model, tokenizer=tokenizer, train_dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fd7ffe3e-dab0-43db-9daf-5179ed556e3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 4142\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1554\n",
      "The following columns in the training set don't have a corresponding argument in `BertForPreTraining.forward` and have been ignored: train. If train are not expected by `BertForPreTraining.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [95]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dev/lib/python3.10/site-packages/transformers/trainer.py:1317\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1312\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1314\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1316\u001b[0m )\n\u001b[0;32m-> 1317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dev/lib/python3.10/site-packages/transformers/trainer.py:1528\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1525\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_rng_state(resume_from_checkpoint)\n\u001b[1;32m   1527\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1528\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1529\u001b[0m \n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;66;03m# Skip past any already trained steps if resuming training\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m steps_trained_in_current_epoch \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1532\u001b[0m         steps_trained_in_current_epoch \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dev/lib/python3.10/site-packages/torch/utils/data/dataloader.py:576\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 576\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dev/lib/python3.10/site-packages/torch/utils/data/dataloader.py:616\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    615\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 616\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    618\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dev/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dev/lib/python3.10/site-packages/transformers/trainer_utils.py:696\u001b[0m, in \u001b[0;36mRemoveColumnsCollator.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[\u001b[38;5;28mdict\u001b[39m]):\n\u001b[1;32m    695\u001b[0m     features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remove_columns(feature) \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[0;32m--> 696\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dev/lib/python3.10/site-packages/transformers/data/data_collator.py:247\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 247\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m    255\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dev/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2795\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   2793\u001b[0m \u001b[38;5;66;03m# The model's main input name, usually `input_ids`, has be passed for padding\u001b[39;00m\n\u001b[1;32m   2794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m encoded_inputs:\n\u001b[0;32m-> 2795\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2796\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2797\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthat includes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but you provided \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(encoded_inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2798\u001b[0m     )\n\u001b[1;32m   2800\u001b[0m required_input \u001b[38;5;241m=\u001b[39m encoded_inputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m   2802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m required_input:\n",
      "\u001b[0;31mValueError\u001b[0m: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided []"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "93653aa5-2c1e-4dd2-870b-98077379422d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'full_determinism'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [96]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBir bahçemiz var bir taraf çiçekli bir [MASK] çöl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dev/lib/python3.10/site-packages/transformers/trainer.py:306\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# Seed must be set before instantiating the model when using model\u001b[39;00m\n\u001b[0;32m--> 306\u001b[0m enable_full_determinism(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mseed) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_determinism\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m set_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mseed)\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhp_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'full_determinism'"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, 'Bir bahçemiz var bir taraf çiçekli bir [MASK] çöl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "447ee627-8759-47da-ae57-c617e0d2c414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.8162245154380798,\n",
       "  'token': 2330,\n",
       "  'token_str': 'taraf',\n",
       "  'sequence': 'bir bahcemiz var bir taraf cicekli bir taraf col'},\n",
       " {'score': 0.11233951151371002,\n",
       "  'token': 8606,\n",
       "  'token_str': 'tarafı',\n",
       "  'sequence': 'bir bahcemiz var bir taraf cicekli bir tarafı col'},\n",
       " {'score': 0.04179617017507553,\n",
       "  'token': 9981,\n",
       "  'token_str': 'tarafta',\n",
       "  'sequence': 'bir bahcemiz var bir taraf cicekli bir tarafta col'},\n",
       " {'score': 0.0095303263515234,\n",
       "  'token': 8007,\n",
       "  'token_str': 'taraftan',\n",
       "  'sequence': 'bir bahcemiz var bir taraf cicekli bir taraftan col'},\n",
       " {'score': 0.006003586109727621,\n",
       "  'token': 10598,\n",
       "  'token_str': 'tarafa',\n",
       "  'sequence': 'bir bahcemiz var bir taraf cicekli bir tarafa col'}]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe('Bir bahçemiz var bir taraf çiçekli bir [MASK] çöl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afe312e8-1b44-4d14-9c40-de7a7224f79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = \n",
    "preprocessed = pipe.preprocess(inputs)\n",
    "model_outputs = pipe.forward(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7355b2c1-42c1-4eaa-aa6c-53ab1de12f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_outputs = []\n",
    "for preprocessed in pipe.preprocess(inputs):\n",
    "    model_outputs = pipe.forward(preprocessed)\n",
    "    all_model_outputs.append(model_outputs)\n",
    "\n",
    "outputs = pipe.postprocess(all_model_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5691cd8b-437d-4019-b340-d46ac9d7d958",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
